---
workflow: validate-user-discovery
type: single-file
description: Validate a user discovery report for research rigor, method appropriateness, and finding quality
author: Isla (discovery-empathy-expert)
---

# Validate User Discovery Report

Bring me a user discovery report and I'll help you assess its research rigor, method appropriateness, and the quality of its findings.

## Why Validation Matters

Discovery research shapes product strategy. If the research is flawed -- biased samples, weak methods, unsupported conclusions -- the strategy built on it will be flawed too. Validation catches these problems before they compound into costly product decisions.

## Validation Process

### 1. Research Rigor Check

**Is the discovery scope well-defined?**
- Clear goal tied to a business or product decision?
- Research questions that are specific and answerable?
- User population defined with inclusion and exclusion criteria?

**Is the evidence base sufficient?**
- Multiple participants (not just 1-2 anecdotes)?
- Multiple methods used (triangulation)?
- Evidence cited for every claim?

**Red Flags:**
- "We believe users want..." (belief is not evidence)
- "Based on our experience..." (team experience is not user research)
- Findings from a single session presented as patterns
- No participant count or sample description
- Conclusions that go far beyond what the data supports

### 2. Method Appropriateness Check

**Were the right methods used for the right questions?**

| Question Type | Strong Methods | Weak Methods |
|--------------|----------------|--------------|
| Understanding workflows | Contextual inquiry, observation | Surveys |
| Measuring prevalence | Surveys, analytics | Interviews alone |
| Discovering motivations | Contextual inquiry, diary studies | Analytics alone |
| Tracking behavior over time | Diary studies, analytics | Single-session interviews |
| Uncovering environment factors | Observation, contextual inquiry | Remote surveys |

**Red Flags:**
- Using surveys to "discover" needs (surveys confirm, they don't discover)
- Drawing motivational conclusions from analytics alone
- Relying entirely on self-reported data without behavioral observation
- Using a single method for all research questions

### 3. Finding Quality Check

**Are themes evidence-based?**
- Each theme supported by data from multiple participants?
- Each theme supported by more than one method?
- Representative quotes that ring true (not cherry-picked)?
- Frequency counts provided (not just "some users")?

**Are opportunity areas grounded?**
- Size signal provided (how many users, how often, how severe)?
- Current alternatives documented (what do users do today)?
- Priority justified by evidence, not gut feeling?

**Red Flags:**
- Themes that are really just topic labels ("Onboarding" vs. "Users skip onboarding and return later when stuck")
- Opportunity areas with no size signal
- Priorities that don't match the evidence strength
- Missing contradictions section (all research has contradictions -- if none are reported, they were probably ignored)

### 4. Bias and Limitations Check

**Sample bias:**
- Were only happy customers studied? (survivorship bias)
- Were participants recruited from a single channel? (selection bias)
- Were certain segments underrepresented?
- Is the sample size adequate for the claims being made?

**Researcher bias:**
- Is there evidence of confirmation bias (only findings that match the hypothesis)?
- Were leading questions used in interviews or surveys?
- Did the same person conduct research AND synthesize findings? (perspective blindness)
- Are interpretations separated from observations?

**Honest limitations:**
- Does the report acknowledge what was NOT learned?
- Are confidence levels realistic?
- Are follow-up research recommendations specific?

### 5. Actionability Check

**Can this report drive decisions?**
- Can a product manager use the opportunity areas to prioritize a roadmap?
- Can a designer use the themes to inform personas or journey maps?
- Can a stakeholder understand the key findings in under 5 minutes?
- Are next steps specific with clear owners?

**Red Flags:**
- Findings are interesting but not actionable ("Users are complex" -- so what?)
- No clear connection between themes and opportunity areas
- Next steps are vague ("Do more research")
- Report reads like an academic paper instead of a decision tool

---

## Your Turn

**Please share:**
1. The user discovery report you want to validate (paste content or provide file path)
2. The original research questions that motivated the discovery
3. Any context about how the research was conducted

I'll review it and provide:
- **Rigor assessment** - Is the evidence base trustworthy?
- **Method critique** - Were the right methods used for the right questions?
- **Finding quality** - Are themes and opportunities well-supported?
- **Bias check** - What blind spots might exist?
- **Actionability score** - Can this report drive real decisions?
- **Validation score** - Overall assessment

---

## Validation Criteria

**Strong Discovery Report:**
- Every theme supported by evidence from 2+ methods
- Opportunity areas include size signals, alternatives, and priority
- Limitations documented honestly
- Sample described with inclusion/exclusion criteria
- Contradictions and outliers discussed
- Actionable for product, design, and strategy teams

**Needs Work:**
- Some themes lack multi-method support
- Opportunity areas missing size signals or priority justification
- Limitations mentioned but not detailed
- Sample described but potential biases not discussed
- Next steps are generic

**Weak:**
- Findings based on single method or tiny sample
- No triangulation between methods
- Claims exceed the evidence
- No limitations section
- Reads as opinions dressed in research language
- Not actionable

---

Ready to validate your discovery report? Share it with me and I'll provide a thorough assessment.
