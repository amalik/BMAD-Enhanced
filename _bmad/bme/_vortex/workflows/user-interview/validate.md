---
validation: user-interview
type: single-file
description: Validate user interview artifacts for evidence quality, question bias, and finding specificity
author: Isla (discovery-empathy-expert)
version: 1.5.0
---

# Validate User Interview Report

**Agent:** Isla (Discovery & Empathy Expert)

**Stream:** Empathize

## Overview

This validation checks that a user interview report meets quality standards for evidence-based research. It ensures insights are grounded in real data, questions avoid bias, and findings are specific enough to act on.

## Validation Checks

### 1. Evidence Quality

**What it checks:** Are insights backed by sufficient, specific evidence from actual interviews?

**Pass criteria:**
- Every insight references at least 2 participant IDs with specific quotes or behaviors
- Evidence includes direct quotes (in quotation marks), not paraphrases
- Counter-evidence is acknowledged where it exists (not hidden)
- Confidence levels are stated and justified
- Saturation is tracked (new findings vs. repeated findings per interview)

**Common failures:**
- Insights with no attributed evidence ("users want X" without saying which users)
- All evidence comes from a single participant (one person's opinion presented as a pattern)
- Only confirming evidence is cited; contradictions are ignored
- Quotes are paraphrased instead of verbatim, introducing interviewer bias

### 2. Question Bias Check

**What it checks:** Does the interview script avoid leading, hypothetical, and binary questions?

**Pass criteria:**
- No core question can be answered with "yes" or "no"
- No question contains value-laden language ("frustrating", "difficult", "amazing")
- No question reveals the desired answer ("Don't you think X would be better?")
- Questions ask about past behavior, not hypothetical future behavior
- No question presupposes a problem ("How frustrated are you with..." assumes frustration)
- Probing questions are included for each core question

**Common failures:**
- "Would you use a feature that does X?" (hypothetical -- untestable)
- "How painful is your current process?" (presupposes pain and leads with "painful")
- "Do you like the dashboard?" (binary -- produces yes/no)
- "We're building X to solve Y. What do you think?" (reveals solution before exploring problem)

### 3. Finding Specificity

**What it checks:** Are findings concrete and actionable, or vague and generic?

**Pass criteria:**
- Findings include specific numbers, frequencies, or timeframes when available
- Pain points describe observable behaviors, not just feelings
- Workarounds are described with enough detail to understand the actual workflow
- Each finding is attributed to a specific participant (P1, P2, etc.)
- Findings distinguish between what participants said vs. what they did

**Common failures:**
- "Users are frustrated" (vague -- which users? about what specifically? how do you know?)
- "Most participants mentioned they'd want feature X" (unspecific -- how many? in response to what question? did they mention it unprompted or in response to a leading question?)
- "The process is slow" (unmeasured -- how slow? compared to what? according to whom?)

### 4. Research Rigor

**What it checks:** Does the overall research process meet minimum quality standards?

**Pass criteria:**
- Research goals are clearly stated and connected to specific decisions
- At least 5 participants were interviewed (minimum for pattern identification)
- Participant screening criteria are documented
- Contradictions between participants are captured and explored
- Research quality self-assessment is honest (acknowledges limitations)
- Script revisions during the research process are documented
- Assumptions from before the research are explicitly compared against findings

**Common failures:**
- Research goals are missing or vague ("understand users better")
- Fewer than 5 interviews with claims of "all users want X"
- No screening criteria documented (unclear who was interviewed and why)
- All findings conveniently align with the team's existing beliefs (confirmation bias)
- No acknowledgment of limitations or areas of uncertainty

### 5. Actionability

**What it checks:** Do the insights lead to concrete, specific next steps?

**Pass criteria:**
- Each insight has a stated implication for the product
- Recommendations are specific enough to act on (not "improve onboarding" but "show value within first 3 minutes by auto-importing existing data")
- There is a clear distinction between "act on now" and "investigate further"
- New questions generated by the research are captured for future investigation
- Recommended next steps include which Vortex workflows to use (e.g., lean-persona, lean-experiment)

**Common failures:**
- Insights without implications ("users do X" -- so what?)
- Generic recommendations ("we should talk to more users")
- No prioritization (everything is equally important, which means nothing is)
- No connection to follow-up research or experimentation

## Validation Scoring

| Check | Weight | Score |
|-------|--------|-------|
| Evidence Quality | 30% | {pass/partial/fail} |
| Question Bias | 20% | {pass/partial/fail} |
| Finding Specificity | 20% | {pass/partial/fail} |
| Research Rigor | 15% | {pass/partial/fail} |
| Actionability | 15% | {pass/partial/fail} |

**Overall:** {PASS / NEEDS REVISION / FAIL}

## Remediation Guidance

**If Evidence Quality fails:**
Go back to raw notes and recordings. Add specific participant IDs, direct quotes, and behavioral observations. Acknowledge counter-evidence you may have glossed over.

**If Question Bias fails:**
Revise the interview script using the bias checklist in Step 2. If interviews are already done, note the bias in your research quality assessment and caveat affected insights.

**If Finding Specificity fails:**
Return to Step 5 and add concrete details: numbers, frequencies, specific tools named, exact workflows described. Replace vague language with observable behaviors.

**If Research Rigor fails:**
If the issue is sample size, conduct additional interviews before synthesizing. If the issue is missing documentation, go back and fill in screening criteria, research goals, and limitations.

**If Actionability fails:**
For each insight, ask: "If a product manager read this, what would they do on Monday morning?" If you can't answer that, the insight needs to be sharpened or connected to a specific decision.

---

**Created with:** BMAD-Enhanced v1.5.0 - Vortex Pattern (Empathize Stream)
**Agent:** Isla (Discovery & Empathy Expert)
**Workflow:** user-interview
