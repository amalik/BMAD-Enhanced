---
step: 6
workflow: learning-card
title: Synthesize
---

# Step 6: Synthesize

Let's bring everything together into the final learning card artifact -- a permanent record of what was tested, what was learned, and what it means.

## Why This Matters

The synthesized learning card serves as:
- An institutional knowledge asset that outlives team changes
- Input for pivot/patch/persevere decisions
- Evidence for stakeholder discussions
- A building block for compounding organizational learning
- A reference point for future experiments on the same topic

## Your Task

Before I generate the final artifact, I need a few synthesis elements:

### 1. One-Sentence Learning Summary

Distill the entire learning card into ONE sentence a busy executive could read:

**Format:** "We [validated/invalidated/partially validated] that [core hypothesis], discovering that [key insight], which means [primary implication]."

**Examples:**
- "We partially validated that distributed teams will adopt async video updates, discovering that adoption requires a team champion role model, which means our onboarding must include champion designation before team rollout."
- "We invalidated that price sensitivity is our biggest conversion barrier, discovering that trust and perceived reliability matter more, which means our landing page should lead with social proof, not discounts."

### 2. Confidence Rating

Rate your overall confidence in the learnings:

| Rating | Meaning |
|--------|---------|
| **HIGH** | Strong evidence, adequate sample, controlled experiment, replicable |
| **MEDIUM** | Directional evidence, some limitations, patterns are clear but not proven |
| **LOW** | Suggestive evidence, significant limitations, needs replication |
| **EXPLORATORY** | Early signal only, hypothesis-generating not hypothesis-confirming |

### 3. Links to Other Learning Cards

Does this learning connect to other experiments or learning cards?

- **Builds on:** Which previous learnings does this extend?
- **Contradicts:** Does this conflict with any previous findings?
- **Enables:** What future experiments does this make possible?

### 4. Tags for Discoverability

Tag the learning card so future team members can find it:

Suggested tag categories:
- **Topic:** (e.g., onboarding, pricing, engagement, retention, acquisition)
- **User segment:** (e.g., remote-managers, enterprise, SMB)
- **Vortex stream:** (e.g., Empathize, Externalize, Systematize)
- **Experiment type:** (e.g., survey, A/B-test, interview, prototype)
- **Outcome:** (e.g., validated, invalidated, inconclusive)

## Example

**One-Sentence Summary:** "We partially validated that distributed teams will adopt async video updates, discovering that a team champion role model drives 25%+ higher adoption than reminders alone, which means our MVP must include champion onboarding as a launch prerequisite."

**Confidence:** MEDIUM - Clear directional patterns from 3 teams/19 participants over 2 weeks, but sample is too small and duration too short for high confidence. Replication planned.

**Links:**
- **Builds on:** Learning Card: "Remote Manager Pain Points" (validated that status tracking is top time-waster)
- **Contradicts:** None
- **Enables:** Follow-up experiments on champion onboarding mechanics, long-term habit retention, pricing for distributed teams

**Tags:** `async-collaboration`, `distributed-teams`, `video-updates`, `adoption`, `Externalize`, `concierge-MVP`, `partially-validated`

---

## Your Turn

Please provide:
1. Your one-sentence learning summary
2. Your confidence rating with justification
3. Any links to related learning cards
4. Tags for discoverability

## Final Step

When you've provided these synthesis elements, I'll:
1. Generate your complete learning card artifact using the template
2. Save it to `{output_folder}/learning-card-{experiment-name}-{date}.md`
3. Highlight any follow-up experiments you should consider
4. Suggest whether this learning is ready to feed into a pivot/patch/persevere decision

---

## Workflow Complete

After synthesis, your learning card artifact will include:
- Complete experiment context and methodology
- Raw results with data quality assessment
- Rigorous analysis with alternative explanations considered
- Validated learnings with confidence ratings
- Strategic implications and recommended actions
- Links to related learnings and discoverability tags
- Revision history for future updates as you learn more

Next suggested workflow: **Max's pivot-patch-persevere** if this learning triggers a strategic decision, or **Wade's lean-experiment** to run follow-up experiments.
