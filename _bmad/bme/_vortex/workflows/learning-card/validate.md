---
workflow: validate-learning-card
type: single-file
description: Validate an existing learning card for evidence quality, learning clarity, and implication actionability
author: Max (learning-decision-expert)
---

# Validate Existing Learning Card

Bring me a learning card and I'll help you assess whether the evidence is rigorous, the learnings are clear, and the implications are actionable.

## Why Validation Matters

Learning cards are only as valuable as the rigor behind them. A learning card that claims "validated" when the evidence is weak leads to bad decisions. A learning card that buries a critical insight in vague language wastes organizational knowledge. Validation ensures your learning cards are trustworthy decision-making inputs.

## Validation Process

### 1. Evidence Quality Check

For each learning statement in the card, assess the evidence:

**Strong Evidence:**
- Pre-defined success criteria existed before the experiment
- Sample size is adequate for the method (5+ interviews per segment, 30+ survey responses, statistical significance for A/B tests)
- Confounding factors are identified and accounted for
- Data quality limitations are acknowledged
- Both quantitative and qualitative data align

**Red Flags:**
- Success criteria were defined AFTER seeing results (moving goalposts)
- Sample size is too small for the claims being made
- Only positive results are reported (cherry-picking)
- No alternative explanations considered
- Quantitative and qualitative data contradict each other without explanation
- "We believe" instead of "The data shows"
- No data quality assessment

### 2. Learning Clarity Check

For each validated learning, verify:

**Is it specific?**
- "Users prefer async" (too vague)
- "Cross-timezone teams of 5-8 achieve 90%+ adoption of async video updates when a team champion records daily" (specific and actionable)

**Is it bounded?**
- Does it state the conditions under which the learning holds?
- Does it acknowledge where it might NOT hold?
- Is the confidence rating appropriate given the evidence?

**Is it honest about uncertainty?**
- Does it distinguish between "validated" and "partially validated"?
- Are inconclusive results marked as such?
- Are new questions raised by the findings documented?

**Is it falsifiable?**
- Could a future experiment prove this learning wrong?
- If not, it's an opinion, not a learning

### 3. Implication Actionability Check

For each implication listed in the card:

**Can someone act on this?**
- "We should improve our onboarding" (not actionable)
- "We should add a champion designation step to team onboarding within the first 48 hours" (actionable)

**Is the logic sound?**
- Does the implication logically follow from the learning?
- Are there leaps in reasoning (learning says X, implication assumes Y)?

**Are the next actions concrete?**
- Do they have clear owners?
- Do they have timeframes?
- Do they have success criteria of their own?

### 4. Completeness Check

**Required Sections:**
- [ ] Experiment context with hypothesis, method, and success criteria
- [ ] Raw results (both quantitative and qualitative)
- [ ] Analysis with pattern identification and alternative explanations
- [ ] Validated learnings with confidence ratings
- [ ] Implications with concrete next actions
- [ ] Evidence quality self-assessment
- [ ] Connection to other learning cards or experiments

**Missing Perspectives:**
- Are dissenting data points captured?
- Are stakeholder concerns acknowledged?
- Is the learning card useful to someone who WASN'T part of the experiment?

---

## Your Turn

**Please share:**
1. The learning card you want to validate (paste content or provide file path)
2. Optionally: the raw experiment data for cross-reference

I'll review it and provide:
- **Evidence assessment** - Is the evidence strong enough to support the claims?
- **Learning clarity score** - Are the learnings specific, bounded, and falsifiable?
- **Implication readiness** - Are the implications actionable and logically grounded?
- **Overall validation rating** - Ready for decisions / Needs revision / Insufficient evidence
- **Specific improvement suggestions** - Exactly what to fix and how

---

## Validation Rating Scale

**Ready for Decisions:**
- Evidence is strong and well-documented
- Learnings are specific, bounded, and appropriately confident
- Implications are actionable with clear next steps
- Limitations are honestly acknowledged
- This learning card can safely inform strategic decisions

**Needs Revision:**
- Evidence has gaps that should be addressed
- Some learnings are vague or overconfident
- Implications don't clearly follow from learnings
- Missing sections or incomplete analysis
- Fix the identified issues before using for decisions

**Insufficient Evidence:**
- Claims exceed what the evidence supports
- Critical methodology flaws (too small sample, no success criteria, severe bias)
- Learnings are opinions dressed as validated findings
- Requires additional experimentation before conclusions can be drawn

---

Ready to validate your learning card? Share it with me and I'll provide a rigorous, honest assessment.
